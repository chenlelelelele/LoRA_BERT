{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 基于梯度敏感度的 Adaptive LoRA 方法\n",
    "\n",
    "### 一、实验任务背景与目标\n",
    "\n",
    "传统 LoRA 方法通常采用统一的 rank（低秩维度）配置，即对所有 Transformer 层的注意力模块分配相同的 LoRA 参数规模。然而，不同层在下游任务中的重要性存在显著差异，统一 rank 可能造成参数浪费或表达能力不足。\n",
    "\n",
    "因此，本实验提出并实现一种 基于梯度敏感度的 Adaptive LoRA 方法，其核心目标是：\n",
    "\n",
    "根据不同 Transformer 层在训练初期的梯度大小，自适应地为各层分配不同的 LoRA rank，从而在保证性能的同时进一步提升参数效率。\n",
    "\n",
    "### 二、整体实验流程概述\n",
    "\n",
    "本实验整体流程可分为五个阶段：\n",
    "\n",
    "加载基础 BERT 模型（不含 LoRA）、Warm-up 阶段统计各层梯度敏感度、基于梯度信息进行 Adaptive Rank 分配、构建 Adaptive LoRA 模型、在 SST-2 数据集上进行正式训练与验证\n"
   ],
   "id": "400b461d23b3588c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T14:02:38.029333Z",
     "start_time": "2025-12-23T13:37:22.274574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =====================================================\n",
    "# Gradient-aware Adaptive LoRA for BERT (SST-2)\n",
    "# 完整修正 & 可直接运行版本\n",
    "# =====================================================\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from collections import defaultdict\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 1. 基本配置\n",
    "# =====================================================\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "MAX_LEN = 128\n",
    "WARMUP_BATCHES = 50\n",
    "TOP_LAYER_RATIO = 0.3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2. 数据集加载（GLUE - SST-2）\n",
    "# =====================================================\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset[\"train\"],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset[\"validation\"],\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3. 梯度敏感度统计（Warm-up）\n",
    "# =====================================================\n",
    "def collect_layer_gradients(model, dataloader, device, num_batches):\n",
    "    \"\"\"\n",
    "    统计 Transformer 每一层 attention query/value 的梯度范数\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    grad_stats = defaultdict(list)\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        if step >= num_batches:\n",
    "            break\n",
    "\n",
    "        # ★ 修正点 1：label -> labels\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        batch[\"labels\"] = batch.pop(\"label\")\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is None:\n",
    "                continue\n",
    "\n",
    "            if \"encoder.layer\" in name and (\n",
    "                \"attention.self.query\" in name or\n",
    "                \"attention.self.value\" in name\n",
    "            ):\n",
    "                # ★ 修正点 2：稳健解析 layer_id\n",
    "                name_parts = name.split(\".\")\n",
    "                if \"layer\" not in name_parts:\n",
    "                    continue\n",
    "\n",
    "                layer_idx = name_parts.index(\"layer\")\n",
    "                if layer_idx + 1 >= len(name_parts):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    layer_id = int(name_parts[layer_idx + 1])\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                grad_stats[layer_id].append(grad_norm)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "    layer_importance = {\n",
    "        layer: sum(vals) / len(vals)\n",
    "        for layer, vals in grad_stats.items()\n",
    "    }\n",
    "\n",
    "    return layer_importance\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4. 选择重要层（Adaptive LoRA）\n",
    "# =====================================================\n",
    "def select_important_layers(layer_importance, top_ratio=0.3):\n",
    "    sorted_layers = sorted(\n",
    "        layer_importance.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    top_k = max(1, int(len(sorted_layers) * top_ratio))\n",
    "    return [layer for layer, _ in sorted_layers[:top_k]]\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 5. 训练 & 验证函数\n",
    "# =====================================================\n",
    "def train_one_epoch(model, loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ★ 修正点 3：label -> labels\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        batch[\"labels\"] = batch.pop(\"label\")\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (preds == batch[\"label\"]).sum().item()\n",
    "            total += batch[\"label\"].size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 6. 参数统计工具\n",
    "# =====================================================\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 7. 主流程\n",
    "# =====================================================\n",
    "print(\"\\n=== Step 1: 初始化基础模型（无 LoRA） ===\")\n",
    "base_model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "print(\"\\n=== Step 2: 梯度敏感度统计（Warm-up） ===\")\n",
    "layer_importance = collect_layer_gradients(\n",
    "    base_model,\n",
    "    train_loader,\n",
    "    device,\n",
    "    WARMUP_BATCHES\n",
    ")\n",
    "\n",
    "print(\"\\nLayer importance (gradient norm):\")\n",
    "for k, v in sorted(layer_importance.items()):\n",
    "    print(f\"Layer {k}: {v:.6f}\")\n",
    "\n",
    "print(\"\\n=== Step 3: 选择重要层 ===\")\n",
    "important_layers = select_important_layers(\n",
    "    layer_importance,\n",
    "    TOP_LAYER_RATIO\n",
    ")\n",
    "print(\"Selected layers:\", important_layers)\n",
    "\n",
    "print(\"\\n=== Step 4: 构建 Adaptive LoRA 模型 ===\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    layers_to_transform=important_layers,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "total_p, trainable_p = count_parameters(model)\n",
    "print(f\"Total Params: {total_p:,}\")\n",
    "print(f\"Trainable Params: {trainable_p:,}\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 8. 优化器 & Scheduler\n",
    "# =====================================================\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 9. 正式训练\n",
    "# =====================================================\n",
    "print(\"\\n=== Step 5: 正式训练 Adaptive LoRA ===\")\n",
    "training_log = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    train_loss = train_one_epoch(\n",
    "        model, train_loader, optimizer, scheduler\n",
    "    )\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    training_log.append((epoch + 1, train_loss, val_acc, epoch_time))\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: \"\n",
    "          f\"Loss={train_loss:.4f}, \"\n",
    "          f\"Val Acc={val_acc:.4f}, \"\n",
    "          f\"Time={epoch_time:.2f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "print(\"Epoch | Train Loss | Val Acc | Time(s)\")\n",
    "for e, l, a, t in training_log:\n",
    "    print(f\"{e:^5} | {l:^10.4f} | {a:^7.4f} | {t:^8.2f}\")\n",
    "\n",
    "print(f\"\\nTotal Training Time: {total_time:.2f}s\")\n",
    "print(\"\\nTraining finished.\")\n"
   ],
   "id": "b6a7b17caf75e58e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== Step 1: 初始化基础模型（无 LoRA） ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 2: 梯度敏感度统计（Warm-up） ===\n",
      "\n",
      "Layer importance (gradient norm):\n",
      "Layer 0: 0.090118\n",
      "Layer 1: 0.079651\n",
      "Layer 2: 0.082389\n",
      "Layer 3: 0.087262\n",
      "Layer 4: 0.088129\n",
      "Layer 5: 0.080186\n",
      "Layer 6: 0.085607\n",
      "Layer 7: 0.077054\n",
      "Layer 8: 0.094206\n",
      "Layer 9: 0.082342\n",
      "Layer 10: 0.083280\n",
      "Layer 11: 0.082088\n",
      "\n",
      "=== Step 3: 选择重要层 ===\n",
      "Selected layers: [8, 0, 4]\n",
      "\n",
      "=== Step 4: 构建 Adaptive LoRA 模型 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 75,266 || all params: 109,559,044 || trainable%: 0.0687\n",
      "Total Params: 109,559,044\n",
      "Trainable Params: 75,266\n",
      "\n",
      "=== Step 5: 正式训练 Adaptive LoRA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4210/4210 [07:54<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.4074, Val Acc=0.8830, Time=478.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4210/4210 [07:54<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.2992, Val Acc=0.8853, Time=477.45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4210/4210 [07:54<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.2896, Val Acc=0.8807, Time=477.19s\n",
      "\n",
      "=== Training Summary ===\n",
      "Epoch | Train Loss | Val Acc | Time(s)\n",
      "  1   |   0.4074   | 0.8830  |  478.07 \n",
      "  2   |   0.2992   | 0.8853  |  477.45 \n",
      "  3   |   0.2896   | 0.8807  |  477.19 \n",
      "\n",
      "Total Training Time: 1432.71s\n",
      "\n",
      "Training finished.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:14:01.887957Z",
     "start_time": "2025-12-23T12:14:01.884937Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4861674c13be0e1b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# LoRA_BERT
随着预训练语言模型规模的不断增大，传统的全参数微调方法在实际应用中面临显著的计算资源和存储开销问题。为解决这一问题，参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）方法逐渐成为自然语言处理领域研究热点。LoRA（Low-Rank Adaptation）作为代表性方法，通过在冻结预训练模型参数的前提下，仅引入少量可训练的低秩矩阵，实现了在保持模型性能的同时显著降低训练成本。
本文以 IMDb 电影评论情感分类任务为研究对象，选取 BERT-base-uncased 作为基础预训练模型，系统研究 LoRA 方法在文本分类任务中的应用效果。实验首先进行了 LoRA rank 消融实验，分析不同 rank 对参数量和分类性能的影响；随后进行了 Full Fine-tuning 对比实验，验证 LoRA 在参数效率和性能上的优势；最后采用 Gradient-aware Adaptive LoRA 策略，根据梯度敏感度选择关键层注入 LoRA，进一步提升参数效率。实验结果表明，LoRA 方法在仅训练不到 0.1% 参数的情况下即可取得接近全参数微调的性能，并且 Adaptive LoRA 方案在保持性能的同时进一步减少训练开销。
项目简介
研究任务背景
文本情感分类是自然语言处理领域中的一项基础任务，其目标是根据文本内容判断作者所表达的情感倾向，通常分为正面和负面两类。该任务在舆情分析、产品评价分析以及推荐系统等实际应用场景中具有重要价值。
近年来，以 BERT 为代表的大规模预训练语言模型在文本分类任务中取得了显著性能提升。然而，随着模型规模不断扩大，传统的全参数微调方法需要对模型中的所有参数进行更新，导致训练成本高、显存占用大，不利于模型在资源受限环境中的部署和应用。
现有方案总结
针对大模型微调成本高的问题，研究者提出了多种参数高效微调方法，包括 Adapter、Prompt Tuning 以及 LoRA 等。这类方法的共同特点是在保持预训练模型主体参数冻结的前提下，仅引入少量可训练参数，从而降低训练和存储开销。
其中，LoRA 方法通过对模型中部分线性变换进行低秩分解，仅训练新增的低秩矩阵参数，在实践中表现出较好的参数效率和性能表现，因此逐渐成为主流的参数高效微调方法之一。
研究动机
尽管 LoRA 方法在多种自然语言处理任务中取得了良好效果，但其在具体任务中 rank 参数的选择仍缺乏统一结论。不同 rank 设置会直接影响可训练参数数量和模型表达能力，因此有必要通过实验对其进行系统分析。
基于此，本文以 IMDb 情感分类任务为例，系统研究 LoRA rank 参数对模型性能和参数效率的影响，并进一步将 LoRA 方法与传统全参数微调方法进行对比，以验证其在实际任务中的有效性。
